{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import faiss\n",
    "from analyze_image import ImageAnalyzer\n",
    "from labels import labels\n",
    "from dataset_creation import DatasetManager, RegexLabelExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSQOuCnkPW8X",
    "tags": []
   },
   "source": [
    "## Data prep for pets data following this [notebook](https://colab.research.google.com/github/akashmehra/blog/blob/fastbook/lessons/_notebooks/2021-07-20-pets_classifier.ipynb#scrollTo=ekNHMAUtklXS)\n",
    "Classes have been moved to dataset_creation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate dataset without train/test split\n",
    "base_img_dir = r'.\\pets\\oxford-iiit-pet\\images'\n",
    "paths = [path for path in sorted(os.listdir(base_img_dir)) if path.endswith('.jpg')]\n",
    "pattern = '(.+)_\\d+.jpg$'\n",
    "regex_label_extractor = RegexLabelExtractor(pattern)\n",
    "dm =  DatasetManager(base_img_dir, paths, regex_label_extractor,\n",
    "                                 seed=42)\n",
    "data=dm.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len = 7390\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Creating FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# taking the output from this snippet and putting in labels.py\n",
    "# df was from Pet Training ResNet34.ipynb\n",
    "\n",
    "# classes = df.to_dict()\n",
    "# class_to_idx = classes['label_name']\n",
    "# class_to_idx\n",
    "# # flip keys and values \n",
    "# labels = {v: k for k, v in class_to_idx.items()}\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "analyze = ImageAnalyzer(data, labels, './artifacts/model_pets.pt')\n",
    "analyze.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create PyTorch dataloader\n",
    "pet_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                                     batch_size = 8,\n",
    "                                     shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get every image's emebedding and put it in the same tensor\n",
    "# should end with shape [7390, 512]\n",
    "all_embeddings = torch.tensor([])\n",
    "for i, batch in enumerate(tqdm(pet_loader)):\n",
    "    for j in range(len(batch[0])):\n",
    "        embed = analyze.getEmbeddings(batch[0][j].unsqueeze(0))\n",
    "        all_embeddings=torch.cat([all_embeddings, embed],0)\n",
    "\n",
    "# save it so we don't have to do it again         \n",
    "torch.save(all_embeddings, \"embeddings_trained_34.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in embeddings from trained ResNet\n",
    "trained_embeddings = torch.load('./artifacts/embeddings_trained_34.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Examining Some of the Learned Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at image embeddings with the highest value for a given index, select the top 10 largest values and output their respective images\n",
    "def display_best_images(feature_index, all_embeddings):\n",
    "    top_ten = sorted(range(len(all_embeddings)), key=lambda k: all_embeddings[k][feature_index].item(),reverse=True)[:10]\n",
    "    top_images = torch.stack([valid_dataset[i][0] for i in top_ten])\n",
    "    analyze.imshow(torchvision.utils.make_grid(top_images, nrow=5, padding=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_best_images(1, trained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_best_images(51, trained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_best_images(101, trained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### A few tests before moving on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_img = Image.open('./test images/ragdoll.jpg')\n",
    "target_transform = analyze.transform(target_img)\n",
    "target_embeddings = analyze.getEmbeddings(target_transform)\n",
    "target_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_cosine = analyze.cosine_similar_images(target_embeddings, trained_embeddings)\n",
    "analyze.show_best_results(top_cosine, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Finally creating the FAISS Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize embeddings\n",
    "faiss.normalize_L2(trained_embeddings.numpy())\n",
    "trained_embeddings\n",
    "# create ids that match those in the dataset\n",
    "ids = np.arange(0, trained_embeddings.shape[0], step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/mdouze/773b2e1b42ac50f700407f3a727921e5\n",
    "# create faiss index\n",
    "# use IP, which will do cosine similarity since the embeddings are normalized\n",
    "dim = trained_embeddings.shape[1]  # 512 features\n",
    "index = faiss.IndexIDMap2(faiss.IndexFlatIP(512))\n",
    "index.add_with_ids(trained_embeddings.numpy(), ids.astype(np.int64))\n",
    "index.is_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faiss.write_index(index, \"pet_faiss_index\")\n",
    "index = faiss.read_index(\"./artifacts/pet_faiss_index\")\n",
    "index.ntotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "distance, indices = index.search(target_embeddings.numpy(), 10)\n",
    "top_images = torch.stack([data[j][0] for j in indices[0]])\n",
    "analyze.imshow(torchvision.utils.make_grid(top_images, nrow = 5, padding = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNGWD2iFYqwhkBJ6dcjaWV2",
   "collapsed_sections": [],
   "mount_file_id": "1EUbOgzzZ06ODugGEoNVd8JXN4-F_NP2Z",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
